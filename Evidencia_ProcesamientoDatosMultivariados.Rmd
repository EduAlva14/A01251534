---
title: "ProcesamientoDatosMultivariados"
author: "Eduardo Alvarado Gómez"
date: "2022-12-04"
output:
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r}
library(MVN)
library(mnormt)
```


# Resumen
Un estudio reciente en 53 lagos de Florida fue realizado con el fin de analizar los factores que más influyen en el nivel de contaminación por mercurio.

Se realizó un análisis de normalidad multivariada en las variables primero, así como uno univariado para verificar cuales variables son normales.

Despúes, se realizó el análisis de componentes principales con la finalidad de encontrar los factores con mayor influencia en la contaminación de mercurio y simplificar el problema.


# Introducción
La contaminación con mercurio de peces de agua dulce comestibles es una amenaza importante contra la salud y se definieron límites de los niveles máximos de Hg de mercurio.

En este reporte se realizó:
- Análisis de normalidad de variables continuas, para identificar variables normales. Esto se hace con el objetivo de ver la diferencia entre la distribución esperada y la observada.
- Análisis de componentes principales con la base de datos completa, para identificar factores principales que intervienen en el problema.


# Análisis de los resultados

## Análisis de normalidad
Análisis de normalidad de variables continuas para identificar variables normales.

Objetivo: ver la diferencia entre la distribución esperada y la observada.

### A. Prueba de normalidad de Mardia y la prueba de Anderson Darling

Con la prueba de Mardia se determina si las variables siguen una distribución normal multivariante. 

$H_0:$ Las variables siguen una distribución normal multivariante.
$H_1:$ Las variables no siguen una distribución normal multivariante.
$\alpha:$ 0.05

Son en total 10 variables (sin la variable de id ya que esta no es necesaria y la variable de edad de peces ya que es categórica).

```{r}
file = "mercurio.csv"
data = read.csv(file)
colnames(data) = c("id", "nombre_lago", "alcalinidad", "ph", "calcio", "clorofila", "concentración_mercurio", "peces", "min_concentración", "max_concentración", "estimación_3_años", "edad_peces")
M = data[, unlist(lapply(data, is.numeric))]
M = subset(M, select = -id)
M = subset(M, select = -edad_peces)
M
```

```{r}
## Test de Multinomalidad: Método Sesgo y kurtosis de Mardia
normality = mvn(M, mvn = "mardia", covariance = FALSE,showOutliers = FALSE)
normality$multivariateNormality
```
Según el análisis, no hay normalidad en los datos, ya que el p value es menor a nuestra $\alpha$ se rechaza la hipótesis nula.

Ya que no existe normalidad multivariada, seguimos con el análisis de la normalidad univariada, para observar si hay variables que tengan distribución normal. 

```{r}
normality$univariateNormality
```
Los resultados de Anderson Darling dicen que las variables con un nivel de significancia mayor a 0.05 no tienen evidencia suficiente para rechazar la hipótesis nula, por lo que podemos decir que los datos sí siguen una distribución normal. 

Las variables con distribución normal:
- ph
- max_concentración


### B. Prueba de normalidad de Mardia y la prueba de Anderson Darling con variables con normalidad y C. Contorno de la normal multivariada
```{r}
M1 = subset(M, select = c(ph, max_concentración))

## Test de Multinomalidad: Método Sesgo y kurtosis de Mardia
mvn(M1,subset = NULL,mvn = "mardia", covariance = FALSE, showOutliers = FALSE, multivariatePlot = "contour")
```

Tenemos los mismos valores de p en esta prueba Anderson que los obtenidos anteriormente.

Sin embargo, ahora ambos valores de sesgo y la curtosis de Mardia son mayores a 0.05. Por esto, no podemos rechazar la hipótesis nula y podemos decir que existe normalidad multivariada.

En la gráfica de los contornos se puede observar que las variables no tienen correlación ya que no están centrados en 0, 0, ni tienen forma circular. 


### D. Detecta datos atípicos o influyentes en la normal multivariada encontrada en el inciso B

```{r}
p = 2 # Número de variables
X = colMeans(M1)
S = cov(M1)

#Distancia de Mahalanobis
d2M =  mahalanobis(M1,X,S)

#Multinormalidad Test gráfico Q-Q Plot
plot(qchisq(((1:nrow(M1)) - 1/2)/nrow(M1),df=p),sort( d2M ), main = "Mahalanobis" )
abline(a=0, b=1,col="red")
```
La distancia Mahalanobis ayuda a medir la distancia entre un punto y una distribución.

Como se puede observar, a medida que aumenta x también lo hace y por lo que no hay valores atípicos.

Hay un outlier que podríamos remover, que el más alejado en la esquina superior derecha.

## Análisis de Componentes Principales

Se realizó un análisis de componentes principales con la base de datos completa para identificar los factores principales que influyen en el problema.

### A. Por qué es adecuado el uso de componentes principales para analizar la base

Es importante usar los componentes principales ya que nos ayuda a seleccionar las características más importantes. Con los componentes principales se reduce el conjunto de variables, manteniendo las que ya no guardan correlación entre si y permitiéndonos reducir la complejidad del problema.

Se usa la matriz de correlaciones debido a que así no se tienen problemas con las diferentes unidades de las variables.

```{r}
corM = cor(M)
cat('\n\nMatriz de correlaciones\n')
```

### B. Análisis de componentes principales y justifica el número de componentes principales apropiados

Obtenemos los valores y los vectores Eigen.

```{r}
ECor = eigen(corM)
eigValCor = ECor$values
eigVecCor = ECor$vectors
cat('\n\nValores y vectores propios de la correlación\n')
ECor
```

Para el primer componente, las variables que más influyen son la 9 y 5, mientras que para el componente 2 las variables que más influyen son la 6 y la 10.

A continuación, calculamos la proporción de varianza explicada por cada componente

```{r}
propCor = eigValCor / sum(diag(corM))
cat('\n\nProporción de varianza explicada de la matriz de correlación\n')
propCor
```

Visualización de resultados

```{r}
resCor = cumsum(propCor)
cat('\n\nAcumulativo de proporción de varianza para la matriz correlación\n')
resCor
```


### C. Representa en un gráfico los vectores asociados a las variables y las puntuaciones de las observaciones de las dos primeras componentes

Podemos ver las puntuaciones de los componentes principales y su explicabilidad.
```{r}
tempCorY = c(0, resCor)
plot(tempCorY,
     type = "b")
```
Llegamos a un nivel de explicación mayor al 90% hasta el 5° componente principal. Los dos primeros componentes explican el 72% de los datos.

```{r}
library(stats)
library(factoextra)
library(ggplot2)
cpSCor = princomp(M, cor=TRUE)
cpaSCor = as.matrix(corM) %*% cpSCor$loadings
plot(cpaSCor[,1:2], type="p", main = "Cor")
text(cpaSCor[,1],cpaSCor[,2],1:nrow(cpaSCor))
biplot(cpSCor)
```
Como muestra la primera gráfica, tenemos dispersión en los datos por lo que no se segmentan en grupos de manera muy definida.

En la segunda gráfica se puede identificar cuáles son las variables con mayor efecto en los componentes principales. Parece que son las variables alcalinidad, calcio y peces, aunque es difícil de determinar con este diagrama.

La gráfica muestra que hay muchas variables en la izquierda, que no se alcanzan a visualizar muy bien.

### D. Interprete los resultados.

```{r}
library(FactoMineR)
library(factoextra)
library(ggplot2) 
cp3 = PCA(M)
fviz_pca_ind(cp3, col.ind = "blue", addEllipses = TRUE, repel = TRUE)
fviz_screeplot(cp3)
fviz_contrib(cp3, choice = c("var"))
```
En esta parte se realiza el PCA (Principal Component Analysis). Con esto se obtiene un análisis más completo y realizado de manera mucho más fácil.

En la **gráfica 1** se observa que los datos son un poco dispersos y no son fácil de agrupar o segmentar, por lo que la explicabilidad de los datos no es buena.

En la **gráfica 2** se observa que las variables que influyen de mayor manera a los componentes principales son:
*  Peces
*  Alcalinidad
*  Ph
*  Calcio

En la **gráfica 3** se observa una elipse con la dispersión de los datos, lo que indica la variablidad de los datos y que hay muy pocos datos que salen fuera de la elipse.

En la **gráfica 4** se observa que el componente 1 y 2 explican poco más del 70% de los datos.

Y finalmente en la **gráfica 5** se observa la influencia de las variables en el componente principal 1. Como se puede ver, las variables que más influyen son las relacionadas con los niveles de mercurio. Las variables que siguen en contribución al modelo son alcalinidad y ph, seguidos de calcio y clorofila.

# Conclusión
Después del análisis, se puede concluir que los factores que más influyen en los niveles de mercurio en el agua son los niveles de alcalinidad y ph encontrados en ella.

Con el análisis multivariado se pudo observar que las variables no eran normales; por eso se realizó el análisis univariado y sacamos las variables que resultaron normales. Después se aplicó PCA solo a las variables que son normales; pero como solo nos quedaron 2 variables multivariadas no sirvió aplicar PCA en ellas. Se aplicó PCA a todas las variables para reducir la dimensionalidad con PCA.

De las gráficas se pudo concluir que el componente principal 1 segmenta dos grupos. Hacia la derecha se encuentra la alcalinidad, clorofila, ph y calcio, y hacia la izquierda se encuentran las variables relacionadas con la concentración de mercurio. El componente principal 2 las segmenta a todas y en otro grupo podemos observar la variable de número de peces.

Se concluye que con los dos primeros componentes ya se pueden explicar todas las variables, por lo que podríamos quedarnos con ellos y sería un análisis aceptable.
